This is a very simple multi-threaded MapReduce framework using goroutines. Data is handled using key-value string pairs at every step. This project includes the basic framework, two example MapReduce use-cases, a simple web-interface framework, and an example program to demonstrate how they are used.

Most of the functionality is implemented through the datatypes.Master struct and associated methods. The data is generated from the Master.input field using the provided Input.GenInput function, and output is similarly handled with the Master.output's associated function fields. Each MapReduce iteration requires a user-defined Job, consisting of a Map function and a Reduce function. In between every pair of adjacent functions, the Master uses Distributor objects to determine which goroutine receives the output data using channels. The default is for a Job's Map function to be associated with a hash-based distributor and its Reduce function to be associated with a round robin-based distributor. Users can define their own distribution functions, but for obvious reasons a Map function must always use a distributor that will select the same channel for each instance of a repeated key.

The Master can be started by calling Run() or by calling Build() followed by Start(). Start() and Run() (which calls start) block until the output has signaled completion. For more information see main.go.

Users are free to define their own distribution functions and input and output functions, but the most common uses are provided in datatypes/builtins.go.
As mentioned above, the provided distribution functions include a round robin distribution (with an optional randomized starting value) and a hash distribution that selects a channel based on the hash of the key.
The first provided input function reads takes a string as a parameter. If the string is a file, it reads the file and outputs each line as a value, using the name of the file and the line number as the key. If the string is a directory, it performs the same process on every file in the directory.
The second provided input function reads from standard in: each line is a value and the key is the line number.
The first provided output function writes the received values to a file, ignoring the key. There are two ways to implement this, using structs or closures. See datatypes/builtins.go for more information.
The second provided output function prints the values to standard out.

The first example finds all cycles of length exactly three in a directed graph, and outputs each cycle exactly once. This example requires two MapReduce iterations. The input must be a graph in adjacency-list representation, where the node is followed by a colon and the edges are separated by commas. Ex: "1:2,3,4" means the node 1 has an edge to the nodes 2, 3, and 4. Technically the node names can be any string except the word "yes", although I suggest using numbers only. See examples/directed_graph.go for more information. The key generated by the input function is ignored.

The second example is a simple inverted index. For each unique value, it will output the incoming keys which held that value. Using either of the provided input functions, it will output the line numbers (and files) that displayed each unique line.

The web interface allows the user to define and run jobs through their browser using the "net/http" go package. The user can change the base directory, input and output, and add layers of MapReduce jobs. The input and output locations are relative to the base directory. The implementing program must first "register" the available constructs by name; the input, output, and MapReduce functions must be available and compiled in order for the program to start. Dynamically loading MapReduce jobs is left as an exercise for the reader.

The example program in main.go allows the user to run the web interface with the '-w' flag, and optionally set the default base directory and input/output locations. The default base directory must be specified before the input/output locations can be specified, and the input/output locations must be specified at the same time or not at all. The current configuration allows the user to select between both provided input functions, both MapReduce examples, and both provided output functions.
Running the example program through the command line interface instead of the web interface will only run the directed graph example using the file input and file output. Like the arguments to the web interface, the base directory must be specified before the input/output locations can be specified, and the input/output locations must be specified at the same time or not at all. Also like the web interface, the input and output locations are relative to the base directory.

Future work:
1. Multi-threaded input and output
Currently, it is "technically" possible to have multi-threaded input and output (such as reading from or writing to multiple files concurrently) by using trivial input and output functions and implementing the real work in MapReduce jobs with some number of concurrent goroutines. However, it would probably be better to build this feature into the framework.
2. Distributed workers
It would be nice to implement distributed workers using sockets to communicate between nodes.
3. Logging
It would be nice to implement logging, using the master to initialize a logging setup similar to the current setup for input and output, and then logging through the workers or even through the MapReduce functions themselves. This could also be used to gather statistics such as number of keys written and read at each layer of the processing, and either output them at the end or update them in real time, possibly through the web interface.
4. Proper testing
I tested the code with a small number of manual test cases. I'm sure there are plenty of bugs. Go has a great framework for building and running automated tests but I didn't take advantage of it.
